import traceback  # For displaying exceptions
import os
import logging
from log_utils import LogUtils
from datetime import datetime  # For current day and time
from datetime import date
from time import time  # For runtime of scripts

from strawberryfields.apps import data, plot, sample, clique
from strawberryfields.apps.sample import postselect
from strawberryfields.decompositions import takagi
from scipy.sparse.csgraph import laplacian
from thewalrus.samples import hafnian_sample_graph
import numpy as np
import networkx as nx
import plotly

#Loading samples generated by Generate_samples.py file
cwd='Results\\01-Jun-2022-(11.28.55.494659)\\'
excel_file=cwd+'nsamples=50000.0_nsubspace=10.0_samples_cov.csv'
tot_samples =np.loadtxt(excel_file,delimiter=',')
#Loading the graph from TaceAs problem
TA = data.TaceAs()
A = TA.adj
print(laplacian(A[:10,:10]))
print(TA.adj)
TA_graph = nx.Graph(A)
TA_graph_laplacian=nx.Graph(laplacian(A))
print(len(tot_samples))
histogram=np.zeros(15)
for s in tot_samples:
    sum=np.sum(s)
    histogram[int(sum)]+=1
print(histogram)

GBS_dens = []
u_dens = []
uniform_samples=[]
postselection_number=2 # Number of photons postselected
samples_GBS=postselect(tot_samples,postselection_number,postselection_number)





##Comparing the difference of density between the 8-nodes subgraphs generated by GBS and a uniform sampling of the graph
for s in samples_GBS:
    uniform = list(np.random.choice(postselection_number,postselection_number, replace=False))  # generates uniform sample
    uniform_samples.append(uniform)
    GBS_dens.append(nx.density(TA_graph.subgraph(s)))
    u_dens.append(nx.density(TA_graph.subgraph(uniform)))

print("GBS mean density = {:.4f}".format(np.mean(GBS_dens)))
print("Uniform mean density = {:.4f}".format(np.mean(u_dens)))

shrunk = [clique.shrink(s, TA_graph) for s in samples_GBS]
print(clique.is_clique(TA_graph.subgraph(shrunk[0])))
print(clique.is_clique(TA_graph.subgraph([0,1,2,8,9])))
clique_sizes = [len(s) for s in shrunk]
print("First ten clique sizes = ", clique_sizes[:10])
print("Average clique size = {:.3f}".format(np.mean(clique_sizes)))
print("Maximum clique size = ", np.max(clique_sizes))
print("Minimum clique size = ", np.min(clique_sizes))

shrunk = [clique.shrink(s, TA_graph) for s in uniform_samples]
print(clique.is_clique(TA_graph.subgraph(shrunk[0])))
clique_sizes=[]
clique_sizes = [len(s) for s in shrunk]
print("First ten clique sizes = ", clique_sizes[:10])
print("Average clique size = {:.3f}".format(np.mean(clique_sizes)))
print("Maximum clique size = ", np.max(clique_sizes))
print("Minimum clique size = ", np.min(clique_sizes))


# searched = [clique.search(s, TA_graph_laplacian, iterations=10) for s in shrunk]
# clique_sizes = [len(s) for s in searched]
# print("First two cliques = ", searched[:2])
# print("Average clique size = {:.3f}".format(np.mean(clique_sizes)))

fig=plot.graph(TA_graph)
fig2=plot.graph(TA_graph_laplacian)
fig.show()
fig2.show()
